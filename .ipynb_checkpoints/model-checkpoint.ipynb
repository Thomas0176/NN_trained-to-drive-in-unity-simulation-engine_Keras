{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "example_img_filename = \"data/IMG/center_2016_12_01_13_30_48_287.jpg\"\n",
    "train_data_folder = \"combined_data/\"\n",
    "csv_filename = \"combined_data/combined_driving_log.csv\"\n",
    "csv_filename_random = \"combined_data/random_combined_driving_log.csv\" #randomized csv file based on csv_filename\n",
    "\n",
    "# train_data_folder = \"recovery_data_ccw/\"\n",
    "# csv_filename = \"recovery_data_ccw/recovery_log.csv\"\n",
    "# csv_filename_random = \"recovery_data_ccw/random_recovery_log.csv\" #randomized csv file based on csv_filename\n",
    "\n",
    "\n",
    "# train_data_folder = \"steering_data/\"\n",
    "# csv_filename = \"steering_data/steering_driving_log.csv\"\n",
    "# csv_filename_random = \"steering_data/random_steering_driving_log.csv\" #randomized csv file based on csv_filename\n",
    "\n",
    "#train_data_folder = \"combined_data/IMG/\"\n",
    "train_images_filenames = os.listdir(train_data_folder)\n",
    "\n",
    "example_img = np.asarray(Image.open(example_img_filename).convert('RGB'))\n",
    "print(example_img.shape)\n",
    "\n",
    "\n",
    "#randomize the loaded csv file\n",
    "def randomize_csv_filename():\n",
    "\n",
    "    with open(csv_filename,'r') as source:\n",
    "        data = [ (random.random(), line) for line in source ]\n",
    "    data.sort()\n",
    "    with open(csv_filename_random,'w') as target:\n",
    "        writer = csv.writer(target)\n",
    "        writer.writerow(['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed'])\n",
    "\n",
    "        firstRow = True\n",
    "        for _, line in data:\n",
    "            if (\"steering\" not in line):\n",
    "                target.write( line )\n",
    "                #print(line)\n",
    "                \n",
    "                \n",
    "randomize_csv_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading the X_images\n",
    "    \n",
    "# for index, filename in enumerate(train_images_filenames):\n",
    "#     print(filename)\n",
    "#     print(\"Reading in the x data...: \", index)\n",
    "#     image = np.asarray(Image.open(train_data_folder+filename).convert('RGB'))\n",
    "    \n",
    "#     X_test_new_img = np.empty(shape=[1, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "#     X_test_new_img = X_test_new_img.astype('uint8')\n",
    "#     X_test_new_img[0] = image\n",
    "#     X_train = np.vstack((X_train, X_test_new_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the y labels\n",
    "# columns: ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
    "# first row = headers\n",
    "\n",
    "# y_train = np.zeros(len(X_train))\n",
    "# y_train = y_train.astype('float64')\n",
    "# loaded_y_column = 3 # column 'steering'\n",
    "\n",
    "# with open(csv_filename, 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "    \n",
    "#     for index, row in enumerate(reader):\n",
    "#         #skip first row of csv as it is just the header\n",
    "        \n",
    "#         if (index != 0):\n",
    "#             #print(train_images_filenames[index-1])\n",
    "#             y_train[index-1]=row[3]\n",
    "#             assert(\"IMG/\"+train_images_filenames[index-1] == row[0]), \"Filename of y is different from filename in x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assert imported data\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "# assert(X_train.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "# assert(X_val.shape[0] == y_val.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "# assert(X_val.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# TODO: Implement data normalization here.\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_val = X_val.astype('float32')\n",
    "# X_train = X_train / 255 - 0.5\n",
    "# X_val = X_val / 255 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert(math.isclose(np.min(X_train), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_train), 0.5, abs_tol=1e-5)), \"The range of the training data is: %.1f to %.1f\" % (np.min(X_train), np.max(X_train))\n",
    "# assert(math.isclose(np.min(X_val), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_val), 0.5, abs_tol=1e-5)), \"The range of the validation data is: %.1f to %.1f\" % (np.min(X_val), np.max(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the neural network with Keras here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 160, 320, 3)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 78, 158, 24)   1824        lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 37, 77, 36)    21636       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 37, 77, 36)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 17, 37, 48)    43248       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 17, 37, 48)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 15, 35, 64)    27712       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 15, 35, 64)    0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 13, 33, 64)    36928       dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 13, 33, 64)    0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 27456)         0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 120)           3294840     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 84)            10164       dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             85          dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,436,437\n",
      "Trainable params: 3,436,437\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation, Convolution2D, Flatten, Dropout, Lambda, Cropping2D\n",
    "# import BatchNormalization\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "# The general use case is to use BN between the linear and non-linear layers in your network, because it normalizes the input to your activation function, so that you're centered in the linear section of the activation function (such as Sigmoid). There's a small discussion of it here\n",
    "\n",
    "\n",
    "\n",
    "#based on NVIDIA selfdriving car network: http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input shape\n",
    "input_shape = (160, 320, 3)\n",
    "# number of convolutional filters to use\n",
    "\n",
    "# In Keras, lambda layers can be used to create arbitrary functions that operate on each image as it passes through the layer.\n",
    "#The lambda layer will also ensure that the model will normalize input images when making predictions in drive.py.\n",
    "#The lambda layer below normalizes that data and the mean centers the data (around 0)\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=input_shape))\n",
    "\n",
    "#crop the image\n",
    "#70 rows pixels from the top of the image\n",
    "#25 rows pixels from the bottom of the image\n",
    "#0 columns of pixels from the left of the image\n",
    "#0 columns of pixels from the right of the image\n",
    "model.add(Cropping2D(cropping=((70,25)(0,0))))\n",
    "\n",
    "#as per feedback:\n",
    "#Considering the above, I would like to encourage you to position the dropout operator between the convolution layers that include many connections.\n",
    "# model.add(BatchNormalization(input_shapee=input_shape))\n",
    "\n",
    "model.add(Convolution2D(24, 5, 5], subsample=(2,2), activation='relu'))\n",
    "model.add(Convolution2D(36, 5, 5], subsample=(2,2), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(48, 5, 5], subsample=(2,2), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(48, 3, 3], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'], lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:  9430\n",
      "samples_per_epoch:  7424\n",
      "nb_val_smaples:  1792\n",
      "Epoch 1/2\n",
      "rangeStart 0\n",
      "rangeEnd 7424\n",
      "Nr. of rows to go through:  7424\n",
      "In batch and firstPass:  True\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1152\n",
      "In batch and firstPass:  False\n",
      "in yield with index  128\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1280\n",
      " 128/7424 [..............................] - ETA: 829s - loss: 0.0910 - mean_squared_error: 0.0910In batch and firstPass:  False\n",
      "in yield with index  256\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1408\n",
      " 256/7424 [>.............................] - ETA: 706s - loss: 0.0749 - mean_squared_error: 0.0749In batch and firstPass:  False\n",
      "in yield with index  384\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1536\n",
      " 384/7424 [>.............................] - ETA: 656s - loss: 0.0639 - mean_squared_error: 0.0639In batch and firstPass:  False\n",
      "in yield with index  512\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1664\n",
      " 512/7424 [=>............................] - ETA: 621s - loss: 0.0596 - mean_squared_error: 0.0596In batch and firstPass:  False\n",
      "in yield with index  640\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1792\n",
      " 640/7424 [=>............................] - ETA: 596s - loss: 0.0537 - mean_squared_error: 0.0537In batch and firstPass:  False\n",
      "in yield with index  768\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1920\n",
      " 768/7424 [==>...........................] - ETA: 577s - loss: 0.0481 - mean_squared_error: 0.0481In batch and firstPass:  False\n",
      "in yield with index  896\n",
      "In batch and firstPass:  False\n",
      "in yield with index  2048\n",
      " 896/7424 [==>...........................] - ETA: 565s - loss: 0.0439 - mean_squared_error: 0.0439In batch and firstPass:  False\n",
      "in yield with index  1024\n",
      "1024/7424 [===>..........................] - ETA: 547s - loss: 0.0408 - mean_squared_error: 0.0408In batch and firstPass:  False\n",
      "in yield with index  1152\n",
      "1152/7424 [===>..........................] - ETA: 526s - loss: 0.0400 - mean_squared_error: 0.0400In batch and firstPass:  False\n",
      "in yield with index  1280\n",
      "1280/7424 [====>.........................] - ETA: 511s - loss: 0.0391 - mean_squared_error: 0.0391In batch and firstPass:  False\n",
      "in yield with index  1408\n",
      "1408/7424 [====>.........................] - ETA: 497s - loss: 0.0371 - mean_squared_error: 0.0371In batch and firstPass:  False\n",
      "in yield with index  1536\n",
      "1536/7424 [=====>........................] - ETA: 484s - loss: 0.0381 - mean_squared_error: 0.0381In batch and firstPass:  False\n",
      "in yield with index  1664\n",
      "1664/7424 [=====>........................] - ETA: 467s - loss: 0.0370 - mean_squared_error: 0.0370In batch and firstPass:  False\n",
      "in yield with index  1792\n",
      "1792/7424 [======>.......................] - ETA: 452s - loss: 0.0368 - mean_squared_error: 0.0368In batch and firstPass:  False\n",
      "in yield with index  1920\n",
      "1920/7424 [======>.......................] - ETA: 437s - loss: 0.0358 - mean_squared_error: 0.0358In batch and firstPass:  False\n",
      "in yield with index  2048\n",
      "2048/7424 [=======>......................] - ETA: 423s - loss: 0.0349 - mean_squared_error: 0.0349In batch and firstPass:  False\n",
      "in yield with index  2176\n",
      "2176/7424 [=======>......................] - ETA: 411s - loss: 0.0347 - mean_squared_error: 0.0347In batch and firstPass:  False\n",
      "in yield with index  2304\n",
      "2304/7424 [========>.....................] - ETA: 398s - loss: 0.0342 - mean_squared_error: 0.0342In batch and firstPass:  False\n",
      "in yield with index  2432\n",
      "2432/7424 [========>.....................] - ETA: 387s - loss: 0.0336 - mean_squared_error: 0.0336In batch and firstPass:  False\n",
      "in yield with index  2560\n",
      "2560/7424 [=========>....................] - ETA: 376s - loss: 0.0335 - mean_squared_error: 0.0335In batch and firstPass:  False\n",
      "in yield with index  2688\n",
      "2688/7424 [=========>....................] - ETA: 365s - loss: 0.0327 - mean_squared_error: 0.0327In batch and firstPass:  False\n",
      "in yield with index  2816\n",
      "2816/7424 [==========>...................] - ETA: 355s - loss: 0.0332 - mean_squared_error: 0.0332In batch and firstPass:  False\n",
      "in yield with index  2944\n",
      "2944/7424 [==========>...................] - ETA: 345s - loss: 0.0332 - mean_squared_error: 0.0332In batch and firstPass:  False\n",
      "in yield with index  3072\n",
      "3072/7424 [===========>..................] - ETA: 334s - loss: 0.0331 - mean_squared_error: 0.0331In batch and firstPass:  False\n",
      "in yield with index  3200\n",
      "3200/7424 [===========>..................] - ETA: 323s - loss: 0.0324 - mean_squared_error: 0.0324In batch and firstPass:  False\n",
      "in yield with index  3328\n",
      "3328/7424 [============>.................] - ETA: 313s - loss: 0.0322 - mean_squared_error: 0.0322In batch and firstPass:  False\n",
      "in yield with index  3456\n",
      "3456/7424 [============>.................] - ETA: 303s - loss: 0.0318 - mean_squared_error: 0.0318In batch and firstPass:  False\n",
      "in yield with index  3584\n",
      "3584/7424 [=============>................] - ETA: 293s - loss: 0.0320 - mean_squared_error: 0.0320In batch and firstPass:  False\n",
      "in yield with index  3712\n",
      "3712/7424 [==============>...............] - ETA: 282s - loss: 0.0316 - mean_squared_error: 0.0316In batch and firstPass:  False\n",
      "in yield with index  3840\n",
      "3840/7424 [==============>...............] - ETA: 273s - loss: 0.0318 - mean_squared_error: 0.0318In batch and firstPass:  False\n",
      "in yield with index  3968\n",
      "3968/7424 [===============>..............] - ETA: 264s - loss: 0.0315 - mean_squared_error: 0.0315In batch and firstPass:  False\n",
      "in yield with index  4096\n",
      "4096/7424 [===============>..............] - ETA: 255s - loss: 0.0311 - mean_squared_error: 0.0311In batch and firstPass:  False\n",
      "in yield with index  4224\n",
      "4224/7424 [================>.............] - ETA: 245s - loss: 0.0308 - mean_squared_error: 0.0308In batch and firstPass:  False\n",
      "in yield with index  4352\n",
      "4352/7424 [================>.............] - ETA: 235s - loss: 0.0307 - mean_squared_error: 0.0307In batch and firstPass:  False\n",
      "in yield with index  4480\n",
      "4480/7424 [=================>............] - ETA: 225s - loss: 0.0305 - mean_squared_error: 0.0305In batch and firstPass:  False\n",
      "in yield with index  4608\n",
      "4608/7424 [=================>............] - ETA: 215s - loss: 0.0306 - mean_squared_error: 0.0306In batch and firstPass:  False\n",
      "in yield with index  4736\n",
      "4736/7424 [==================>...........] - ETA: 206s - loss: 0.0308 - mean_squared_error: 0.0308In batch and firstPass:  False\n",
      "in yield with index  4864\n",
      "4864/7424 [==================>...........] - ETA: 195s - loss: 0.0309 - mean_squared_error: 0.0309In batch and firstPass:  False\n",
      "in yield with index  4992\n",
      "4992/7424 [===================>..........] - ETA: 186s - loss: 0.0307 - mean_squared_error: 0.0307In batch and firstPass:  False\n",
      "in yield with index  5120\n",
      "5120/7424 [===================>..........] - ETA: 176s - loss: 0.0308 - mean_squared_error: 0.0308In batch and firstPass:  False\n",
      "in yield with index  5248\n",
      "5248/7424 [====================>.........] - ETA: 166s - loss: 0.0308 - mean_squared_error: 0.0308In batch and firstPass:  False\n",
      "in yield with index  5376\n",
      "5376/7424 [====================>.........] - ETA: 156s - loss: 0.0306 - mean_squared_error: 0.0306In batch and firstPass:  False\n",
      "in yield with index  5504\n",
      "5504/7424 [=====================>........] - ETA: 147s - loss: 0.0308 - mean_squared_error: 0.0308In batch and firstPass:  False\n",
      "in yield with index  5632\n",
      "5632/7424 [=====================>........] - ETA: 137s - loss: 0.0305 - mean_squared_error: 0.0305In batch and firstPass:  False\n",
      "in yield with index  5760\n",
      "5760/7424 [======================>.......] - ETA: 127s - loss: 0.0305 - mean_squared_error: 0.0305In batch and firstPass:  False\n",
      "in yield with index  5888\n",
      "5888/7424 [======================>.......] - ETA: 118s - loss: 0.0308 - mean_squared_error: 0.0308In batch and firstPass:  False\n",
      "in yield with index  6016\n",
      "6016/7424 [=======================>......] - ETA: 108s - loss: 0.0309 - mean_squared_error: 0.0309In batch and firstPass:  False\n",
      "in yield with index  6144\n",
      "6144/7424 [=======================>......] - ETA: 98s - loss: 0.0308 - mean_squared_error: 0.0308 In batch and firstPass:  False\n",
      "in yield with index  6272\n",
      "6272/7424 [========================>.....] - ETA: 88s - loss: 0.0306 - mean_squared_error: 0.0306In batch and firstPass:  False\n",
      "in yield with index  6400\n",
      "6400/7424 [========================>.....] - ETA: 78s - loss: 0.0305 - mean_squared_error: 0.0305In batch and firstPass:  False\n",
      "in yield with index  6528\n",
      "6528/7424 [=========================>....] - ETA: 69s - loss: 0.0306 - mean_squared_error: 0.0306In batch and firstPass:  False\n",
      "in yield with index  6656\n",
      "6656/7424 [=========================>....] - ETA: 59s - loss: 0.0308 - mean_squared_error: 0.0308In batch and firstPass:  False\n",
      "in yield with index  6784\n",
      "6784/7424 [==========================>...] - ETA: 49s - loss: 0.0306 - mean_squared_error: 0.0306In batch and firstPass:  False\n",
      "in yield with index  6912\n",
      "6912/7424 [==========================>...] - ETA: 39s - loss: 0.0304 - mean_squared_error: 0.0304In batch and firstPass:  False\n",
      "in yield with index  7040\n",
      "7040/7424 [===========================>..] - ETA: 29s - loss: 0.0302 - mean_squared_error: 0.0302In batch and firstPass:  False\n",
      "in yield with index  7168\n",
      "7168/7424 [===========================>..] - ETA: 19s - loss: 0.0303 - mean_squared_error: 0.0303"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def generate_arrays_from_file(split_start, split_end, validationSet):\n",
    "    while 1: \n",
    "        with open(csv_filename_random, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            firstPass = True\n",
    "            \n",
    "            rangeStart = int(split_start*row_count) ## 0 for train set\n",
    "            \n",
    "            if validationSet:\n",
    "                rangeEnd = rangeStart + nb_val_samples\n",
    "            else:\n",
    "                rangeEnd = rangeStart + samples_per_epoch\n",
    "            #time.sleep(5)\n",
    "            print(\"rangeStart\", rangeStart)\n",
    "            print(\"rangeEnd\", rangeEnd)\n",
    "            print (\"Nr. of rows to go through: \", rangeEnd-rangeStart)\n",
    "            \n",
    "#             X_data = np.empty(shape=[batch_size, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "#             X_data[None,:,:,:] = X_data[None,:,:,:].astype('float32')\n",
    "#             y_data = np.zeros(batch_size)\n",
    "#             y_data = y_data.astype('float32')\n",
    "#             index_xy = 0\n",
    "                \n",
    "\n",
    "            for index, row in enumerate(reader):\n",
    "                \n",
    "                # range (rangeStart, rangeEnd):\n",
    "                if (index!= 0 and index >= rangeStart and index <= rangeEnd):\n",
    "#                     print (\"index :\", index)\n",
    "                    #random_row_index = index\n",
    "                    \n",
    "#                     random_row_index = random.randrange(rangeStart+1, rangeEnd) #don't use first row\n",
    "#                     print (\"Random row index: \", random_row_index)\n",
    "\n",
    "\n",
    "    #                 #get the random row;\n",
    "    #                 for i, row in enumerate(reader):\n",
    "    #                     if i == random_row_index:\n",
    "    #                         random_row = row\n",
    "    #                         break\n",
    "\n",
    "                    #enter every batch size, incl. first pass\n",
    "                    if ((index % batch_size) == 0 or firstPass):\n",
    "                        print(\"In batch and firstPass: \", firstPass)\n",
    "\n",
    "                        #don't yield during first pass\n",
    "                        if index != 0 and not firstPass:\n",
    "                            print (\"in yield with index \", index)\n",
    "    #                       STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "                            assert(X_data.shape[0] == y_data.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "                            assert(X_data.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "    #                       STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "#                             assert(math.isclose(np.min(X_data), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_data), 0.5, abs_tol=1e-5)), \"The range of the training data is: %.1f to %.1f\" % (np.min(X_data), np.max(X_data))\n",
    "                            yield X_data, y_data\n",
    "                        X_data = np.empty(shape=[batch_size, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "    #                     X_data = X_data.astype('uint8')\n",
    "    #                     X_data[None,:,:,:] = X_data[None,:,:,:].astype('uint8')\n",
    "                        X_data[None,:,:,:] = X_data[None,:,:,:].astype('float32')\n",
    "\n",
    "                        y_data = np.zeros(batch_size)\n",
    "                        y_data = y_data.astype('float32')\n",
    "                        index_xy = 0\n",
    "\n",
    "#                     print(\"filepath: \", train_data_folder+row[0])\n",
    "                    image = np.asarray(Image.open(train_data_folder+row[0]).convert('RGB'))\n",
    "    #                 image = image.astype('uint8')\n",
    "#                     time.sleep(1)\n",
    "\n",
    "            \n",
    "                    \n",
    "                    #normalizing the image\n",
    "                    image = image.astype('float32')\n",
    "#                   image = image/255-0.5  \n",
    "                    #image = image - 128 \n",
    "                    \n",
    "                    # X_data[None,:,:,:].astype('uint8')\n",
    "                    X_data[index_xy]= image\n",
    "                    X_data = X_data.astype('float32')\n",
    "                    y_data[index_xy]=row[3]\n",
    "                    firstPass = False\n",
    "                    index_xy += 1\n",
    "\n",
    "#                     print (\"\\n random row filename: \", row[0])\n",
    "#                     print(\"\\n steering angle: \", y_data[index_xy])\n",
    "\n",
    "\n",
    "    #                 print(\"X_data shape\")\n",
    "    #                 print(X_data.shape)\n",
    "    #                 print(\"y_data\")\n",
    "    #                 print(y_data.shape)\n",
    "\n",
    "    #                 print(\"This is the line.\")\n",
    "    #                 print(random_row[3])\n",
    "#     #                 print(\"This is the filename\")\n",
    "#     #                 print(str(train_images_filenames[random_row_index-1]))\n",
    "#                     print(\"This is index_xy\")\n",
    "#                     print(index_xy)\n",
    "    #                 print(\"This is index_xy shap eof Xtest new img\")\n",
    "    #                 print(X_data[index_xy].shape)\n",
    "    #                 print(type(X_data[index_xy]))\n",
    "    #                 print(type(image))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #                 plt.title(str(random_row(0)))\n",
    "    #                 plt.imshow(image)\n",
    "    #                 plt.show()\n",
    "#                     time.sleep(1) # delays for 5 seconds\n",
    "\n",
    "#                     plt.imshow(X_data[index_xy])\n",
    "#                     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "with open(csv_filename_random, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        row_count = sum(1 for row in reader) \n",
    "        print (\"Row count: \", row_count)\n",
    "\n",
    "nb_epoch=2\n",
    "train_perc= 0.8 ##split between training and validation set. 1-train_perc = validation_perc\n",
    "batch_size = 128 #128 #7936\n",
    "\n",
    "samples_per_epoch = batch_size * int(row_count*train_perc/batch_size) ##ensure that (samples per epoch)%(batch_size) == 0 \n",
    "print(\"samples_per_epoch: \", samples_per_epoch)\n",
    "nb_val_samples = batch_size * int(row_count*(1-train_perc)/batch_size)\n",
    "print(\"nb_val_smaples: \", nb_val_samples)\n",
    "\n",
    "#int(row_count*train_perc)\n",
    "# history = model.fit_generator(generate_arrays_from_file(0,train_perc), samples_per_epoch=100, nb_epoch=1, verbose=1, validation_data=generate_arrays_from_file(val_perc, 1),\n",
    "# nb_val_samples=row_count*val_perc)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generate_arrays_from_file(0,train_perc, False),\n",
    "    samples_per_epoch=samples_per_epoch,\n",
    "    nb_epoch=nb_epoch, \n",
    "    #verbose =1)\n",
    "    verbose=1,\n",
    "    validation_data=generate_arrays_from_file(train_perc, 1, True),\n",
    "    nb_val_samples=nb_val_samples) \n",
    "\n",
    "# history = model.fit_generator(generate_arrays_from_file(), samples_per_epoch=1000, nb_epoch=2, verbose=1)\n",
    "\n",
    "# history = model.fit_generator(generate_arrays_from_file(), samples_per_epoch=10000, nb_epoch=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SAVE THE MODEL\n",
    "# serialize model to JSON\n",
    "model_json_simple = model.to_json()\n",
    "with open(\"model_simple.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json_simple)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_simple.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "\n",
    "print \"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
