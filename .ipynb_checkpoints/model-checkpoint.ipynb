{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "            \n",
    "example_img_filename = \"data/IMG/center_2016_12_01_13_30_48_287.jpg\"\n",
    "train_data_folder = \"steering_data/IMG/\"\n",
    "csv_filename = \"steering_data/edit_driving_log.csv\"\n",
    "train_images_filenames = os.listdir(train_data_folder)\n",
    "\n",
    "example_img = np.asarray(Image.open(example_img_filename).convert('RGB'))\n",
    "print(example_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading the X_images\n",
    "    \n",
    "# for index, filename in enumerate(train_images_filenames):\n",
    "#     print(filename)\n",
    "#     print(\"Reading in the x data...: \", index)\n",
    "#     image = np.asarray(Image.open(train_data_folder+filename).convert('RGB'))\n",
    "    \n",
    "#     X_test_new_img = np.empty(shape=[1, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "#     X_test_new_img = X_test_new_img.astype('uint8')\n",
    "#     X_test_new_img[0] = image\n",
    "#     X_train = np.vstack((X_train, X_test_new_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the y labels\n",
    "# columns: ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
    "# first row = headers\n",
    "\n",
    "# y_train = np.zeros(len(X_train))\n",
    "# y_train = y_train.astype('float64')\n",
    "# loaded_y_column = 3 # column 'steering'\n",
    "\n",
    "# with open(csv_filename, 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "    \n",
    "#     for index, row in enumerate(reader):\n",
    "#         #skip first row of csv as it is just the header\n",
    "        \n",
    "#         if (index != 0):\n",
    "#             #print(train_images_filenames[index-1])\n",
    "#             y_train[index-1]=row[3]\n",
    "#             assert(\"IMG/\"+train_images_filenames[index-1] == row[0]), \"Filename of y is different from filename in x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assert imported data\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "# assert(X_train.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "# assert(X_val.shape[0] == y_val.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "# assert(X_val.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# TODO: Implement data normalization here.\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_val = X_val.astype('float32')\n",
    "# X_train = X_train / 255 - 0.5\n",
    "# X_val = X_val / 255 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert(math.isclose(np.min(X_train), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_train), 0.5, abs_tol=1e-5)), \"The range of the training data is: %.1f to %.1f\" % (np.min(X_train), np.max(X_train))\n",
    "# assert(math.isclose(np.min(X_val), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_val), 0.5, abs_tol=1e-5)), \"The range of the validation data is: %.1f to %.1f\" % (np.min(X_val), np.max(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the neural network with Keras here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 78, 158, 24)   1824        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 37, 77, 36)    21636       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 17, 37, 48)    43248       convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 15, 35, 64)    27712       convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 13, 33, 64)    36928       convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 27456)         0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 27456)         0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 27456)         0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           2745700     activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            5050        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            510         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             11          dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,882,619\n",
      "Trainable params: 2,882,619\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation, Convolution2D, Flatten, Dropout\n",
    "\n",
    "#based on NVIDIA selfdriving car network: http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input shape\n",
    "input_shape = (160, 320, 3)\n",
    "# number of convolutional filters to use\n",
    "nb_filters_1 = 24\n",
    "nb_filters_2 = 36\n",
    "nb_filters_3 = 48\n",
    "nb_filters_4 = 64\n",
    "nb_filters_5 = 64\n",
    "kernel_size_1 = (5, 5)\n",
    "kernel_size_2 = (3, 3)\n",
    "subsample_1=(2, 2)\n",
    "subsample_2=(1, 1)\n",
    "\n",
    "#as per feedback:\n",
    "#Considering the above, I would like to encourage you to position the dropout operator between the convolution layers that include many connections.\n",
    "\n",
    "model.add(Convolution2D(nb_filters_1, kernel_size_1[0], kernel_size_1[1], subsample=subsample_1, border_mode='valid', input_shape=input_shape, activation='relu'))\n",
    "model.add(Convolution2D(nb_filters_2, kernel_size_1[0], kernel_size_1[1], subsample=subsample_1, border_mode='valid', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(nb_filters_3, kernel_size_1[0], kernel_size_1[1], subsample=subsample_1, border_mode='valid', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(nb_filters_4, kernel_size_2[0], kernel_size_2[1], subsample=subsample_2, border_mode='valid', input_shape=input_shape, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(nb_filters_5, kernel_size_2[0], kernel_size_2[1], subsample=subsample_2, border_mode='valid', input_shape=input_shape, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:  3676\n",
      "Epoch 1/1Nr. of row: \n",
      " 2426\n",
      "In batch\n",
      "In batch\n",
      "in yield with index  126\n",
      " 126/2426 [>.............................] - ETA: 158s - loss: 0.0143 - mean_squared_error: 0.0143In batch\n",
      "in yield with index  252\n",
      " 252/2426 [==>...........................] - ETA: 120s - loss: 4.2565 - mean_squared_error: 4.2565In batch\n",
      "in yield with index  378\n",
      " 378/2426 [===>..........................] - ETA: 106s - loss: 2.8772 - mean_squared_error: 2.8772In batch\n",
      "in yield with index  504\n",
      " 504/2426 [=====>........................] - ETA: 98s - loss: 2.1594 - mean_squared_error: 2.1594 In batch\n",
      "in yield with index  630\n",
      " 630/2426 [======>.......................] - ETA: 89s - loss: 1.7313 - mean_squared_error: 1.7313In batch\n",
      "in yield with index  756\n",
      " 756/2426 [========>.....................] - ETA: 82s - loss: 1.4431 - mean_squared_error: 1.4431In batch\n",
      "in yield with index  882\n",
      " 882/2426 [=========>....................] - ETA: 75s - loss: 1.2476 - mean_squared_error: 1.2476In batch\n",
      "in yield with index  1008\n",
      "1008/2426 [===========>..................] - ETA: 68s - loss: 1.0918 - mean_squared_error: 1.0918In batch\n",
      "in yield with index  1134\n",
      "1134/2426 [=============>................] - ETA: 61s - loss: 0.9731 - mean_squared_error: 0.9731In batch\n",
      "in yield with index  1260\n",
      "1260/2426 [==============>...............] - ETA: 55s - loss: 0.8785 - mean_squared_error: 0.8785In batch\n",
      "in yield with index  1386\n",
      "1386/2426 [================>.............] - ETA: 49s - loss: 0.8000 - mean_squared_error: 0.8000In batch\n",
      "in yield with index  1512\n",
      "1512/2426 [=================>............] - ETA: 43s - loss: 0.7340 - mean_squared_error: 0.7340In batch\n",
      "in yield with index  1638\n",
      "1638/2426 [===================>..........] - ETA: 37s - loss: 0.6777 - mean_squared_error: 0.6777In batch\n",
      "in yield with index  1764\n",
      "1764/2426 [====================>.........] - ETA: 31s - loss: 0.6293 - mean_squared_error: 0.6293In batch\n",
      "in yield with index  1890\n",
      "1890/2426 [======================>.......] - ETA: 25s - loss: 0.5875 - mean_squared_error: 0.5875In batch\n",
      "in yield with index  2016\n",
      "2016/2426 [=======================>......] - ETA: 19s - loss: 0.5511 - mean_squared_error: 0.5511In batch\n",
      "in yield with index  2142\n",
      "2142/2426 [=========================>....] - ETA: 13s - loss: 0.5188 - mean_squared_error: 0.5188In batch\n",
      "in yield with index  2268\n",
      "2268/2426 [===========================>..] - ETA: 7s - loss: 0.4900 - mean_squared_error: 0.4900 In batch\n",
      "in yield with index  2394\n",
      "Nr. of row:  2426\n",
      "In batch\n",
      "2394/2426 [============================>.] - ETA: 1s - loss: 0.4643 - mean_squared_error: 0.4643In batch\n",
      "in yield with index  126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tvdo/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1537: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of row:  3676\n",
      "In batch\n",
      "In batch\n",
      "in yield with index  1260\n",
      "In batch\n",
      "in yield with index  252\n",
      "In batch\n",
      "in yield with index  1386\n",
      "In batch\n",
      "in yield with index  378\n",
      "In batch\n",
      "in yield with index  1512\n",
      "In batch\n",
      "in yield with index  504\n",
      "In batch\n",
      "in yield with index  1638\n",
      "In batch\n",
      "in yield with index  630\n",
      "In batch\n",
      "in yield with index  1764\n",
      "In batch\n",
      "in yield with index  756\n",
      "In batch\n",
      "in yield with index  1890\n",
      "In batch\n",
      "in yield with index  882\n",
      "In batch\n",
      "in yield with index  2016\n",
      "In batch\n",
      "in yield with index  1008\n",
      "In batch\n",
      "in yield with index  2142\n",
      "In batch\n",
      "in yield with index  1134\n",
      "In batch\n",
      "in yield with index  2268\n",
      "In batch\n",
      "in yield with index  1260\n",
      "In batch\n",
      "in yield with index  2394\n",
      "In batch\n",
      "in yield with index  1386\n",
      "In batch\n",
      "in yield with index  2520\n",
      "2520/2426 [===============================] - 183s - loss: 0.4416 - mean_squared_error: 0.4416 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_arrays_from_file(split_start, split_end):\n",
    "    while 1: \n",
    "        with open(csv_filename, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "        \n",
    "            #iterate row_count times \n",
    "            print (\"Nr. of row: \", int(split_end*row_count))\n",
    "            firstPass = True\n",
    "            \n",
    "            \n",
    "            for index in range (int(split_start*row_count), int(split_end*row_count)):\n",
    "                random_row_index = random.randrange(int(split_start*row_count)+1, int(split_end*row_count)) #don't use first row\n",
    "#                 print (\"Random row index: \", random_row_index)\n",
    "\n",
    "                \n",
    "                #get the random row;\n",
    "                for i, row in enumerate(reader):\n",
    "                    if i == random_row_index:\n",
    "                        random_row = row\n",
    "                        break\n",
    "\n",
    "                #enter every batch size, incl. first pass\n",
    "                if ((index % batch_size) == 0 or firstPass):\n",
    "                    print(\"In batch\")\n",
    "\n",
    "                    #don't yield during first pass\n",
    "                    if index != 0 and not firstPass:\n",
    "\n",
    "                        print (\"in yield with index \", index)\n",
    "#                         STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "                        assert(X_data.shape[0] == y_data.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "                        assert(X_data.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "\n",
    "#                       STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "                        assert(math.isclose(np.min(X_data), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_data), 0.5, abs_tol=1e-5)), \"The range of the training data is: %.1f to %.1f\" % (np.min(X_data), np.max(X_data))\n",
    "                        yield X_data, y_data\n",
    "                    X_data = np.empty(shape=[batch_size, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "#                     X_data = X_data.astype('uint8')\n",
    "#                     X_data[None,:,:,:] = X_data[None,:,:,:].astype('uint8')\n",
    "                    X_data[None,:,:,:] = X_data[None,:,:,:].astype('float32')\n",
    "\n",
    "                    y_data = np.zeros(batch_size)\n",
    "                    y_data = y_data.astype('float32')\n",
    "                    index_xy = 0\n",
    "                \n",
    "\n",
    "                image = np.asarray(Image.open(train_data_folder+train_images_filenames[random_row_index-1]).convert('RGB'))\n",
    "#                 image = image.astype('uint8')\n",
    "                image = image.astype('float32')\n",
    "                image = image/255-0.5                   \n",
    "#                 X_data[None,:,:,:].astype('uint8')\n",
    "                X_data[index_xy]= image\n",
    "                X_data = X_data.astype('float32')\n",
    "                y_data[index_xy]=random_row[3]\n",
    "                firstPass = False\n",
    "\n",
    "                \n",
    "                \n",
    "#                 print(\"X_data shape\")\n",
    "#                 print(X_data.shape)\n",
    "#                 print(\"y_data\")\n",
    "#                 print(y_data.shape)\n",
    "                \n",
    "#                 print(\"This is the line.\")\n",
    "#                 print(random_row[3])\n",
    "#                 print(\"This is the filename\")\n",
    "#                 print(str(train_images_filenames[random_row_index-1]))\n",
    "#                 print(\"This is index_xy\")\n",
    "#                 print(index_xy)\n",
    "#                 print(\"This is index_xy shap eof Xtest new img\")\n",
    "#                 print(X_data[index_xy].shape)\n",
    "#                 print(type(X_data[index_xy]))\n",
    "#                 print(type(image))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 plt.title(str(train_images_filenames[random_row_index-1]))\n",
    "#                 plt.imshow(image)\n",
    "#                 plt.show()\n",
    "#                 time.sleep(1) # delays for 5 seconds\n",
    "\n",
    "#                 plt.imshow(X_data[index_xy])\n",
    "#                 plt.show()\n",
    "                \n",
    "                index_xy += 1\n",
    "\n",
    "\n",
    "batch_size = 126 #128 #7936\n",
    "with open(csv_filename, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        row_count = sum(1 for row in reader) \n",
    "        print (\"Row count: \", row_count)\n",
    "\n",
    "train_perc=0.66\n",
    "val_perc = 1-train_perc\n",
    "\n",
    "# history = model.fit_generator(generate_arrays_from_file(0,train_perc), samples_per_epoch=100, nb_epoch=1, verbose=1, validation_data=generate_arrays_from_file(val_perc, 1),\n",
    "# nb_val_samples=row_count*val_perc)\n",
    "\n",
    "history = model.fit_generator(generate_arrays_from_file(0,train_perc), samples_per_epoch=int(row_count*train_perc), nb_epoch=1, verbose=1, validation_data=generate_arrays_from_file(val_perc, 1),\n",
    "nb_val_samples=row_count*val_perc)\n",
    "\n",
    "# history = model.fit_generator(generate_arrays_from_file(), samples_per_epoch=1000, nb_epoch=2, verbose=1)\n",
    "\n",
    "# history = model.fit_generator(generate_arrays_from_file(), samples_per_epoch=10000, nb_epoch=10, verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#SAVE THE MODEL\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0db122c0bacb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloaded_model_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# load weights into new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "\n",
    "print \"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
