{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "example_img_filename = \"data/IMG/center_2016_12_01_13_30_48_287.jpg\"\n",
    "# train_data_folder = \"combined_data/\"\n",
    "# csv_filename = \"combined_data/combined_driving_log.csv\"\n",
    "# csv_filename_random = \"combined_data/random_combined_driving_log.csv\" #randomized csv file based on csv_filename\n",
    "\n",
    "\n",
    "train_data_folder = \"recovery_data_ccw/\"\n",
    "csv_filename = \"recovery_data_ccw/recovery_log.csv\"\n",
    "csv_filename_random = \"recovery_data_ccw/random_recovery_log.csv\" #randomized csv file based on csv_filename\n",
    "\n",
    "\n",
    "# train_data_folder = \"steering_data/\"\n",
    "# csv_filename = \"steering_data/steering_driving_log.csv\"\n",
    "# csv_filename_random = \"steering_data/random_steering_driving_log.csv\" #randomized csv file based on csv_filename\n",
    "\n",
    "#train_data_folder = \"combined_data/IMG/\"\n",
    "train_images_filenames = os.listdir(train_data_folder)\n",
    "\n",
    "example_img = np.asarray(Image.open(example_img_filename).convert('RGB'))\n",
    "print(example_img.shape)\n",
    "\n",
    "\n",
    "#randomize the loaded csv file\n",
    "def randomize_csv_filename():\n",
    "\n",
    "    with open(csv_filename,'r') as source:\n",
    "        data = [ (random.random(), line) for line in source ]\n",
    "    data.sort()\n",
    "    with open(csv_filename_random,'w') as target:\n",
    "        writer = csv.writer(target)\n",
    "        writer.writerow(['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed'])\n",
    "\n",
    "        firstRow = True\n",
    "        for _, line in data:\n",
    "            if (\"steering\" not in line):\n",
    "                target.write( line )\n",
    "                #print(line)\n",
    "                \n",
    "                \n",
    "randomize_csv_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading the X_images\n",
    "    \n",
    "# for index, filename in enumerate(train_images_filenames):\n",
    "#     print(filename)\n",
    "#     print(\"Reading in the x data...: \", index)\n",
    "#     image = np.asarray(Image.open(train_data_folder+filename).convert('RGB'))\n",
    "    \n",
    "#     X_test_new_img = np.empty(shape=[1, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "#     X_test_new_img = X_test_new_img.astype('uint8')\n",
    "#     X_test_new_img[0] = image\n",
    "#     X_train = np.vstack((X_train, X_test_new_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the y labels\n",
    "# columns: ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
    "# first row = headers\n",
    "\n",
    "# y_train = np.zeros(len(X_train))\n",
    "# y_train = y_train.astype('float64')\n",
    "# loaded_y_column = 3 # column 'steering'\n",
    "\n",
    "# with open(csv_filename, 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "    \n",
    "#     for index, row in enumerate(reader):\n",
    "#         #skip first row of csv as it is just the header\n",
    "        \n",
    "#         if (index != 0):\n",
    "#             #print(train_images_filenames[index-1])\n",
    "#             y_train[index-1]=row[3]\n",
    "#             assert(\"IMG/\"+train_images_filenames[index-1] == row[0]), \"Filename of y is different from filename in x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assert imported data\n",
    "\n",
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert(X_train.shape[0] == y_train.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "# assert(X_train.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "# assert(X_val.shape[0] == y_val.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "# assert(X_val.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# TODO: Implement data normalization here.\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_val = X_val.astype('float32')\n",
    "# X_train = X_train / 255 - 0.5\n",
    "# X_val = X_val / 255 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "# assert(math.isclose(np.min(X_train), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_train), 0.5, abs_tol=1e-5)), \"The range of the training data is: %.1f to %.1f\" % (np.min(X_train), np.max(X_train))\n",
    "# assert(math.isclose(np.min(X_val), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_val), 0.5, abs_tol=1e-5)), \"The range of the validation data is: %.1f to %.1f\" % (np.min(X_val), np.max(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the neural network with Keras here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 160, 320, 3)   12          batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 78, 158, 24)   1824        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 37, 77, 36)    21636       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 37, 77, 36)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 17, 37, 48)    43248       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 17, 37, 48)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 15, 35, 64)    27712       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 15, 35, 64)    0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 13, 33, 64)    36928       dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 13, 33, 64)    0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 27456)         0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 27456)         0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           2745700     activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            5050        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            510         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             11          dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,882,631\n",
      "Trainable params: 2,882,625\n",
      "Non-trainable params: 6\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation, Convolution2D, Flatten, Dropout\n",
    "# import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "# The general use case is to use BN between the linear and non-linear layers in your network, because it normalizes the input to your activation function, so that you're centered in the linear section of the activation function (such as Sigmoid). There's a small discussion of it here\n",
    "\n",
    "\n",
    "\n",
    "#based on NVIDIA selfdriving car network: http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input shape\n",
    "input_shape = (160, 320, 3)\n",
    "# number of convolutional filters to use\n",
    "nb_filters_1 = 24\n",
    "nb_filters_2 = 36\n",
    "nb_filters_3 = 48\n",
    "nb_filters_4 = 64\n",
    "nb_filters_5 = 64\n",
    "kernel_size_1 = (5, 5)\n",
    "kernel_size_2 = (3, 3)\n",
    "subsample_1=(2, 2)\n",
    "subsample_2=(1, 1)\n",
    "\n",
    "#as per feedback:\n",
    "#Considering the above, I would like to encourage you to position the dropout operator between the convolution layers that include many connections.\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "model.add(Convolution2D(nb_filters_1, kernel_size_1[0], kernel_size_1[1], subsample=subsample_1, border_mode='valid', activation='relu'))\n",
    "\n",
    "model.add(Convolution2D(nb_filters_2, kernel_size_1[0], kernel_size_1[1], subsample=subsample_1, border_mode='valid', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(nb_filters_3, kernel_size_1[0], kernel_size_1[1], subsample=subsample_1, border_mode='valid', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(nb_filters_4, kernel_size_2[0], kernel_size_2[1], subsample=subsample_2, border_mode='valid', input_shape=input_shape, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(nb_filters_5, kernel_size_2[0], kernel_size_2[1], subsample=subsample_2, border_mode='valid', input_shape=input_shape, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'], lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count:  1394\n",
      "samples_per_epoch:  896\n",
      "nb_val_smaples:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tvdo/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1907: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1rangeStart\n",
      " 0\n",
      "rangeEnd 896\n",
      "Nr. of rows to go through:  896\n",
      "In batch and firstPass:  True\n",
      "In batch and firstPass:  False\n",
      "in yield with index  128\n",
      "128/896 [===>..........................] - ETA: 62s - loss: 1.2880 - mean_squared_error: 1.2880In batch and firstPass:  False\n",
      "in yield with index  256\n",
      "256/896 [=======>......................] - ETA: 40s - loss: 614.0697 - mean_squared_error: 614.0697In batch and firstPass:  False\n",
      "in yield with index  384\n",
      "384/896 [===========>..................] - ETA: 29s - loss: 416.2130 - mean_squared_error: 416.2130In batch and firstPass:  False\n",
      "in yield with index  512\n",
      "512/896 [================>.............] - ETA: 21s - loss: 314.2449 - mean_squared_error: 314.2449In batch and firstPass:  False\n",
      "in yield with index  640\n",
      "640/896 [====================>.........] - ETA: 14s - loss: 255.2118 - mean_squared_error: 255.2118In batch and firstPass:  False\n",
      "in yield with index  768\n",
      "768/896 [========================>.....] - ETA: 6s - loss: 215.6779 - mean_squared_error: 215.6779 In batch and firstPass:  False\n",
      "in yield with index  896\n",
      "rangeStart 0\n",
      "rangeEnd 896\n",
      "Nr. of rows to go through:  896\n",
      "In batch and firstPass:  True\n",
      "rangeStart 920\n",
      "rangeEnd 1304\n",
      "Nr. of rows to go through:  384\n",
      "In batch and firstPass:  True\n",
      "In batch and firstPass:  False\n",
      "in yield with index  128\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1024\n",
      "In batch and firstPass:  False\n",
      "in yield with index  256\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1152\n",
      "In batch and firstPass:  False\n",
      "in yield with index  384\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1280\n",
      "rangeStart 920\n",
      "rangeEnd 1304\n",
      "Nr. of rows to go through:  384\n",
      "In batch and firstPass:  True\n",
      "In batch and firstPass:  False\n",
      "in yield with index  512\n",
      "In batch and firstPass:  False\n",
      "in yield with index  1024\n",
      "896/896 [==============================] - 75s - loss: 186.8798 - mean_squared_error: 186.8798 - val_loss: 0.0830 - val_mean_squared_error: 0.0830\n",
      "In batch and firstPass:  False\n",
      "in yield with index  640\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def generate_arrays_from_file(split_start, split_end, validationSet):\n",
    "    while 1: \n",
    "        with open(csv_filename_random, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            firstPass = True\n",
    "            \n",
    "            rangeStart = int(split_start*row_count) ## 0 for train set\n",
    "            \n",
    "            if validationSet:\n",
    "                rangeEnd = rangeStart + nb_val_samples\n",
    "            else:\n",
    "                rangeEnd = rangeStart + samples_per_epoch\n",
    "            #time.sleep(5)\n",
    "            print(\"rangeStart\", rangeStart)\n",
    "            print(\"rangeEnd\", rangeEnd)\n",
    "            print (\"Nr. of rows to go through: \", rangeEnd-rangeStart)\n",
    "            \n",
    "#             X_data = np.empty(shape=[batch_size, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "#             X_data[None,:,:,:] = X_data[None,:,:,:].astype('float32')\n",
    "#             y_data = np.zeros(batch_size)\n",
    "#             y_data = y_data.astype('float32')\n",
    "#             index_xy = 0\n",
    "                \n",
    "\n",
    "            for index, row in enumerate(reader):\n",
    "                \n",
    "                # range (rangeStart, rangeEnd):\n",
    "                if (index!= 0 and index >= rangeStart and index <= rangeEnd):\n",
    "#                     print (\"index :\", index)\n",
    "                    #random_row_index = index\n",
    "                    \n",
    "#                     random_row_index = random.randrange(rangeStart+1, rangeEnd) #don't use first row\n",
    "#                     print (\"Random row index: \", random_row_index)\n",
    "\n",
    "\n",
    "    #                 #get the random row;\n",
    "    #                 for i, row in enumerate(reader):\n",
    "    #                     if i == random_row_index:\n",
    "    #                         random_row = row\n",
    "    #                         break\n",
    "\n",
    "                    #enter every batch size, incl. first pass\n",
    "                    if ((index % batch_size) == 0 or firstPass):\n",
    "                        print(\"In batch and firstPass: \", firstPass)\n",
    "\n",
    "                        #don't yield during first pass\n",
    "                        if index != 0 and not firstPass:\n",
    "                            print (\"in yield with index \", index)\n",
    "    #                       STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "                            assert(X_data.shape[0] == y_data.shape[0]), \"The number of images is not equal to the number of labels.\"\n",
    "                            assert(X_data.shape[1:] == (160,320,3)), \"The dimensions of the images are not 32 x 32 x 3.\"\n",
    "    #                       STOP: Do not change the tests below. Your implementation should pass these tests. \n",
    "                            assert(math.isclose(np.min(X_data), -0.5, abs_tol=1e-5) and math.isclose(np.max(X_data), 0.5, abs_tol=1e-5)), \"The range of the training data is: %.1f to %.1f\" % (np.min(X_data), np.max(X_data))\n",
    "                            yield X_data, y_data\n",
    "                        X_data = np.empty(shape=[batch_size, example_img.shape[0], example_img.shape[1], example_img.shape[2]])\n",
    "    #                     X_data = X_data.astype('uint8')\n",
    "    #                     X_data[None,:,:,:] = X_data[None,:,:,:].astype('uint8')\n",
    "                        X_data[None,:,:,:] = X_data[None,:,:,:].astype('float32')\n",
    "\n",
    "                        y_data = np.zeros(batch_size)\n",
    "                        y_data = y_data.astype('float32')\n",
    "                        index_xy = 0\n",
    "\n",
    "#                     print(\"filepath: \", train_data_folder+row[0])\n",
    "                    image = np.asarray(Image.open(train_data_folder+row[0]).convert('RGB'))\n",
    "    #                 image = image.astype('uint8')\n",
    "#                     time.sleep(1)\n",
    "\n",
    "            \n",
    "# print (\"\\n current row filename: \", train_images_filenames[random_row_index-1] )\n",
    "\n",
    "                    image = image.astype('float32')\n",
    "                    image = image/255-0.5                   \n",
    "    #                 X_data[None,:,:,:].astype('uint8')\n",
    "                    X_data[index_xy]= image\n",
    "                    X_data = X_data.astype('float32')\n",
    "                    y_data[index_xy]=row[3]\n",
    "                    firstPass = False\n",
    "                    index_xy += 1\n",
    "\n",
    "#                     print (\"\\n random row filename: \", row[0])\n",
    "#                     print(\"\\n steering angle: \", y_data[index_xy])\n",
    "\n",
    "\n",
    "    #                 print(\"X_data shape\")\n",
    "    #                 print(X_data.shape)\n",
    "    #                 print(\"y_data\")\n",
    "    #                 print(y_data.shape)\n",
    "\n",
    "    #                 print(\"This is the line.\")\n",
    "    #                 print(random_row[3])\n",
    "#     #                 print(\"This is the filename\")\n",
    "#     #                 print(str(train_images_filenames[random_row_index-1]))\n",
    "#                     print(\"This is index_xy\")\n",
    "#                     print(index_xy)\n",
    "    #                 print(\"This is index_xy shap eof Xtest new img\")\n",
    "    #                 print(X_data[index_xy].shape)\n",
    "    #                 print(type(X_data[index_xy]))\n",
    "    #                 print(type(image))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #                 plt.title(str(random_row(0)))\n",
    "    #                 plt.imshow(image)\n",
    "    #                 plt.show()\n",
    "#                     time.sleep(1) # delays for 5 seconds\n",
    "\n",
    "#                     plt.imshow(X_data[index_xy])\n",
    "#                     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "with open(csv_filename_random, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        row_count = sum(1 for row in reader) \n",
    "        print (\"Row count: \", row_count)\n",
    "\n",
    "nb_epoch=1\n",
    "train_perc=0.66 ##split between training and validation set. 1-train_perc = validation_perc\n",
    "batch_size = 128 #128 #7936\n",
    "\n",
    "samples_per_epoch = batch_size * int(row_count*train_perc/batch_size) ##ensure that (samples per epoch)%(batch_size) == 0 \n",
    "print(\"samples_per_epoch: \", samples_per_epoch)\n",
    "nb_val_samples = batch_size * int(row_count*(1-train_perc)/batch_size)\n",
    "print(\"nb_val_smaples: \", nb_val_samples)\n",
    "\n",
    "#int(row_count*train_perc)\n",
    "# history = model.fit_generator(generate_arrays_from_file(0,train_perc), samples_per_epoch=100, nb_epoch=1, verbose=1, validation_data=generate_arrays_from_file(val_perc, 1),\n",
    "# nb_val_samples=row_count*val_perc)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generate_arrays_from_file(0,train_perc, False),\n",
    "    samples_per_epoch=samples_per_epoch,\n",
    "    nb_epoch=nb_epoch, \n",
    "    #verbose =1)\n",
    "    verbose=1,\n",
    "    validation_data=generate_arrays_from_file(train_perc, 1, True),\n",
    "    nb_val_samples=nb_val_samples)\n",
    "\n",
    "# history = model.fit_generator(generate_arrays_from_file(), samples_per_epoch=1000, nb_epoch=2, verbose=1)\n",
    "\n",
    "# history = model.fit_generator(generate_arrays_from_file(), samples_per_epoch=10000, nb_epoch=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#SAVE THE MODEL\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "\n",
    "print \"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
